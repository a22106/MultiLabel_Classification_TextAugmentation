{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf-gpu-cuda8': conda)",
   "metadata": {
    "interpreter": {
     "hash": "592d2365cf8c69d9089d1741d1cf6e338245e4159eff9ececf626c2bb7405064"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "!pip install nlpaug"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 571kB/s]\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "### Char Augmenter\n",
    "# Optical character recognition\n",
    "aug_char_OCR = nac.OcrAug()\n",
    "# Keyboard\n",
    "aug_char_Keyboard = nac.KeyboardAug()\n",
    "\n",
    "### Word Augmenter\n",
    "# Spelling\n",
    "aug_word_Spelling = naw.SpellingAug()\n",
    "#aug_word_WordEmbedng = naw.WordEmbsAug(model_type = 'word2vec', model_path=model_dir+'GoogleNews-vectors-negative300.bin', action=\"insert\")\n",
    "#aug_word_TfIdfAug = naw.TfIdfAug(model_path = os.environ.get(\"MODEL_DIR\"), action = 'insert')\n",
    "aug_word_ContextualWordEmbs = naw.ContextualWordEmbsAug(model_path = 'bert-base-uncased', action = 'insert')\n",
    "aug_word_Synonym = naw.SynonymAug(aug_src = 'wordnet')\n",
    "aug_word_Antonym = naw.AntonymAug()\n",
    "aug_word_Split = naw.SplitAug()"
   ]
  },
  {
   "source": [
    "# 원본 데이터 정제"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_data(dataset_name):\n",
    "    # 원본 HADOOP csv데이터 불러와서 정제하기\n",
    "    issueReport = pd.read_csv(\"../Dataset/Deepsoft_IssueData/{}.csv\".format(dataset_name))\n",
    "    issueOnehot = issueReport.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "\n",
    "    issueLabel = []\n",
    "    for i in range(len(issueOnehot)):\n",
    "        issueLabel.append(list(issueOnehot.iloc[i]))\n",
    "\n",
    "    # make 'data' value\n",
    "    refined_data = issueReport\n",
    "\n",
    "    issueText = pd.Series(list(issueReport[\"title\"] + ' ' + issueReport['description']), index = refined_data.index)\n",
    "    refined_data['text'] = issueText\n",
    "\n",
    "    refined_data['labels'] = pd.Series(issueLabel, index = refined_data.index)\n",
    "\n",
    "    refined_data = refined_data.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "    refined_data = pd.DataFrame(data = {'text':issueText, 'labels': issueLabel})\n",
    "    return refined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_data(refined_data):\n",
    "    # 정제된 데이터 text 리스트화\n",
    "    data_list = []\n",
    "    len(refined_data['text'])\n",
    "    for x in range(len(refined_data['text'])):\n",
    "        data_list.append(refined_data['text'][x])\n",
    "    return data_list\n",
    "\n",
    "#AUG_INTERVAL = 2\n"
   ]
  },
  {
   "source": [
    "# 원본 데이터 augmentation\n",
    "f = open('HADOOP_aug_mul3.csv', 'a', newline='', encoding='utf-8')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(data)\n",
    "for x in range(len(data)):\n",
    "    wr.writerow([data['text'][x], data['labels'][x]])\n",
    "\n",
    "for line in range(AUG_RANGE):\n",
    "    augmented_Embeded = aug_Embeded.augment(data['text'][line])\n",
    "    wr.writerow([augmented_Embeded, data['labels'][line]])\n",
    "f.close()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(dataset_name, dataset, aug_target, augmenter, augmenter_name, augment_times = 6):\n",
    "    # augmentation 데이터 추가\n",
    "    if augment_times <= 0:\n",
    "        return\n",
    "\n",
    "    aug_range = len(dataset)\n",
    "    f = open('../Dataset/Deepsoft_IssueData_AUG/{}_{}_{}.csv'.format(dataset_name, aug_target, augmenter_name), 'w', newline='', encoding='utf-8')\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow(dataset)\n",
    "    for x in range(len(dataset)):\n",
    "        wr.writerow([dataset['text'][x], dataset['labels'][x]])\n",
    "    \n",
    "    for x in range(augment_times): # aug multip\n",
    "        for line in range(aug_range):\n",
    "            augmented_Embeded = augmenter.augment(dataset['text'][line])\n",
    "            wr.writerow([augmented_Embeded, dataset['labels'][line]])\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refine_data, augment_data\n",
    "\n",
    "dataset_HADOOP = refine_data(\"HADOOP\")\n",
    "dataset_FCREPO = refine_data(\"FCREPO\")\n",
    "dataset_ISLANDORA = refine_data(\"ISLANDORA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text  \\\n",
       "0     External triplestore with additional capabilit...   \n",
       "1     BDef and BMech Modification : with Integrity A...   \n",
       "2     Logging Refactoring : consistency and enable e...   \n",
       "3     Proper Architecture of Multiple Triplestore Op...   \n",
       "4     Content Model Architecture (CMA) Date: 2006-09...   \n",
       "...                                                 ...   \n",
       "1648  Update fcrepo-indexing-solr to use fcrepo-ldpa...   \n",
       "1649  Remove extra pom file Remove unneeded pom file...   \n",
       "1650  Importer needs to create repository resources ...   \n",
       "1651  Ensure Link Headers not too long This task is ...   \n",
       "1652  Verify dates makes sense when performing impor...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "1648  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1649  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1650  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1651  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1652  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1653 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>External triplestore with additional capabilit...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>BDef and BMech Modification : with Integrity A...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Logging Refactoring : consistency and enable e...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Proper Architecture of Multiple Triplestore Op...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Content Model Architecture (CMA) Date: 2006-09...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1648</th>\n      <td>Update fcrepo-indexing-solr to use fcrepo-ldpa...</td>\n      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1649</th>\n      <td>Remove extra pom file Remove unneeded pom file...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1650</th>\n      <td>Importer needs to create repository resources ...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1651</th>\n      <td>Ensure Link Headers not too long This task is ...</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1652</th>\n      <td>Verify dates makes sense when performing impor...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1653 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "dataset_FCREPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"HADOOP\", dataset_HADOOP, 'char', aug_char_OCR, \"OCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"HADOOP\", dataset_HADOOP, 'char', aug_char_Keyboard, \"Keyboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"HADOOP\", dataset_HADOOP, 'word', aug_word_Spelling, \"Spelling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "augment_data(\"HADOOP\", dataset_HADOOP, 'word', aug_word_ContextualWordEmbs, \"ContextualWordEmbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"HADOOP\", dataset_HADOOP, 'word', aug_word_Synonym, \"Synonym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"HADOOP\", dataset_HADOOP, 'word', aug_word_Antonym, \"Antonym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"HADOOP\", dataset_HADOOP, 'word', aug_word_Split, \"Split\")"
   ]
  },
  {
   "source": [
    "# FCREPO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"FCREPO\", dataset_FCREPO, 'char', aug_char_OCR, \"OCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"FCREPO\", dataset_FCREPO, 'char', aug_char_Keyboard, \"Keyboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"FCREPO\", dataset_FCREPO, 'word', aug_word_Spelling, \"Spelling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"FCREPO\", dataset_FCREPO, 'word', aug_word_ContextualWordEmbs, \"ContextualWordEmbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"FCREPO\", dataset_FCREPO, 'word', aug_word_Synonym, \"Synonym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"FCREPO\", dataset_FCREPO, 'word', aug_word_Antonym, \"Antonym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"FCREPO\", dataset_FCREPO, 'word', aug_word_Split, \"Split\")"
   ]
  },
  {
   "source": [
    "# ISLANDORA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"ISLANDORA\", dataset_ISLANDORA, 'char', aug_char_OCR, \"OCR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"ISLANDORA\", dataset_ISLANDORA, 'char', aug_char_Keyboard, \"Keyboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"ISLANDORA\", dataset_ISLANDORA, 'word', aug_word_Spelling, \"Spelling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"ISLANDORA\", dataset_ISLANDORA, 'word', aug_word_ContextualWordEmbs, \"ContextualWordEmbs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"ISLANDORA\", dataset_ISLANDORA, 'word', aug_word_Synonym, \"Synonym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"ISLANDORA\", dataset_ISLANDORA, 'word', aug_word_Antonym, \"Antonym\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_data(\"ISLANDORA\", dataset_ISLANDORA, 'word', aug_word_Split, \"Split\")"
   ]
  }
 ]
}