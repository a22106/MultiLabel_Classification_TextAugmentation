{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf-gpu-cuda8': conda)",
   "metadata": {
    "interpreter": {
     "hash": "592d2365cf8c69d9089d1741d1cf6e338245e4159eff9ececf626c2bb7405064"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "import os\n",
    "\n",
    "import inspect\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "import json, re, nltk, string\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDL_LABEL_NUM = 89\n",
    "JRA_LABEL_NUM = 142\n",
    "ISLANDORA_LABEL_NUM = 67\n",
    "INFRA_LABEL_NUM = 51\n",
    "HIVE_LABEL_NUM = 65\n",
    "HBASE_LABEL_NUM = 68\n",
    "HADOOP_LABEL_NUM = 37\n",
    "FCREPO_LABEL_NUM = 22\n",
    "CONF_LABEL_NUM = 128\n",
    "CB_LABEL_NUM = 64\n",
    "CASSANDRA_LABEL_NUM = 15\n",
    "BAM_LABEL_NUM = 96\n",
    "\n",
    "labels_Num = {'MDL': MDL_LABEL_NUM, \n",
    "    'JRA': JRA_LABEL_NUM, 'ISLANDORA': ISLANDORA_LABEL_NUM, \n",
    "    'INFRA': INFRA_LABEL_NUM, 'HIVE': HIVE_LABEL_NUM, 'HBASE': HBASE_LABEL_NUM, 'HADOOP': HADOOP_LABEL_NUM, 'FCREPO': FCREPO_LABEL_NUM, 'CONF': CONF_LABEL_NUM,\n",
    "    'CB': CB_LABEL_NUM, 'CASSANDRA': CASSANDRA_LABEL_NUM, 'BAM': BAM_LABEL_NUM\n",
    "    }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_Classification:\n",
    "    def __init__(self, dataset_name, augmenter_name, augment_size, nlp_model_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.labels_num = labels_Num[dataset_name]\n",
    "\n",
    "        if augmenter_name == 'OCR' or augmenter_name == 'Keyboard':\n",
    "            self.augmentation_type = 'char'\n",
    "        else:\n",
    "            self.augmentation_type = 'word'\n",
    "\n",
    "        self.augmenter_name = augmenter_name\n",
    "        self.aug_mul = augment_size\n",
    "        self.nlp_model_name = nlp_model_name\n",
    "        self.nlp_model = {'bert': 'bert-base-uncased', 'roberta': 'roberta-base', 'xlnet': 'xlnet-base-uncased', 'distilbert': 'distilbert-base-uncased', 'xlm': 'xlm-roberta-base', 'electra': 'google/electra-base-discriminator'}\n",
    "\n",
    "\n",
    "        # 데이터 위치 data location\n",
    "        self.data_location_ori = '../Dataset/Deepsoft_IssueData/{}.csv'.format(self.dataset_name)\n",
    "\n",
    "        self.data_location_aug = '../Dataset/Deepsoft_IssueData_Aug/{}_{}_{}.csv'.format(self.dataset_name, self.augmentation_type, self.augmenter_name)     \n",
    "        \n",
    "\n",
    "        # 데이터 변수 입력\n",
    "        self.data = pd.read_csv(self.data_location_aug) # 증강 데이터\n",
    "        self.data_ori = pd.read_csv(self.data_location_ori) # 원본 데이터\n",
    "        self.len_data = len(self.data_ori)\n",
    "        self.eval_index = []\n",
    "        self.test_index = []\n",
    "\n",
    "\n",
    "\n",
    "    def refine_origin_data(self):\n",
    "        data_ori = self.data_ori\n",
    "        \n",
    "        data_onehot = data_ori.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "        data_label = []\n",
    "        for i in range(len(data_onehot)):\n",
    "            data_label.append(list(data_onehot.iloc[i]))\n",
    "\n",
    "        # make 'data' value\n",
    "        data_text = pd.Series(list(data_ori[\"title\"] + ' ' + data_ori['description']), index = data_ori.index)\n",
    "        #data = data.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "        \n",
    "\n",
    "        refined_data = []\n",
    "        for item in data_text:\n",
    "            #1. Remove \\r \n",
    "            current_desc = item.replace('\\r', ' ')    \n",
    "            #2. Remove URLs\n",
    "            current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "            #4. Remove hex code\n",
    "            current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc) \n",
    "            #5. Change to lower case\n",
    "            current_desc = current_desc.lower()   \n",
    "            #6. Tokenize\n",
    "            #current_desc_tokens = tokenizer(current_desc, add_special_tokens= True)\n",
    "            #7. Strip trailing punctuation marks    \n",
    "            #current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]     \n",
    "            #8. Join the lists\n",
    "            #current_data = current_desc_filter\n",
    "            #current_data = list(filter(None, current_data))\n",
    "            refined_data.append(current_desc)\n",
    "\n",
    "        #data_ori = pd.DataFrame(data = {'text': refined_data, 'labels': data_label})\n",
    "        data_ori['text'] = refined_data\n",
    "        data_ori['labels'] = data_label\n",
    "        self.data_ori = data_ori\n",
    "        # 오리지날 데이터를 train, eval데이터로 분리\n",
    "\n",
    "        train_size = 0.6\n",
    "        test_size = 0.2\n",
    "        eval_size = 0.2\n",
    "        self.train_data_ori, self.eval_data_ori = train_test_split(data_ori, train_size = train_size)\n",
    "        self.eval_data_ori, self.test_data_ori = train_test_split(self.eval_data_ori, test_size = 0.5)\n",
    "        self.eval_data = self.eval_data_ori\n",
    "        self.train_data = self.train_data_ori\n",
    "        self.test_data = self.test_data_ori\n",
    "\n",
    "\n",
    "    \n",
    "    # 불러온 정제된 데이터 one hot을 str에서 list로 바꾸는 작업\n",
    "    def labels_to_int(self):\n",
    "        if self.aug_mul <= 1:\n",
    "            return        \n",
    "\n",
    "        data = self.data[: self.len_data * self.aug_mul] \n",
    "\n",
    "        refined_data = []\n",
    "        for item in data['text']:\n",
    "            #1. Remove \\r \n",
    "            current_desc = item.replace('\\r', ' ')    \n",
    "            #2. Remove URLs\n",
    "            current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "            #4. Remove hex code\n",
    "            current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc) \n",
    "            #5. Change to lower case\n",
    "            current_desc = current_desc.lower()   \n",
    "            #6. Tokenize\n",
    "            #current_desc_tokens = tokenizer(current_desc, add_special_tokens= True)\n",
    "            #7. Strip trailing punctuation marks    \n",
    "            #current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]     \n",
    "            #8. Join the lists\n",
    "            #current_data = current_desc_filter\n",
    "            #current_data = list(filter(None, current_data))\n",
    "            refined_data.append(current_desc)\n",
    "\n",
    "        data['text'] = refined_data\n",
    "\n",
    "        changeChar = ' [],'\n",
    "        for i in range(len(data)):\n",
    "            for chanChar in changeChar:\n",
    "                data['labels'][i] = data['labels'][i].replace(chanChar, '')\n",
    "            data['labels'][i] = list(data['labels'][i])\n",
    "            data['labels'][i] = list(map(int, data['labels'][i]))\n",
    "        \n",
    "        # 증강데이터의 train_data에서 evaluation부분 제거\n",
    "        eval_index_list = list(self.eval_data.index)\n",
    "        test_index_list = list(self.test_data.index)\n",
    "        \n",
    "        for aug_num in range(self.aug_mul):\n",
    "            iidf2 = [i + self.len_data* aug_num for i in eval_index_list]\n",
    "            self.eval_index = self.eval_index + iidf2\n",
    "            iidf3 = [x + self.len_data* aug_num for x in test_index_list]\n",
    "            self.test_index = self.test_index + iidf3\n",
    "\n",
    "\n",
    "        self.data = data\n",
    "        self.train_data = data.drop(self.eval_index)\n",
    "        self.train_data = self.train_data.drop(self.test_index)\n",
    "        self.train_data = self.train_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 모델 parameter 설정\n",
    "    def set_model(self): # epochs: 200, batch size: 100, learning rate 0.002\n",
    "        self.model = MultiLabelClassificationModel(self.nlp_model_name, self.nlp_model[self.nlp_model_name], num_labels = self.labels_num, \n",
    "        args = {'output_dir': '/data/a22106/Deepsoft_C_Multilabel/{}_{}_{}_{}/'.format(self.dataset_name, self.nlp_model_name, self.augmenter_name, self.aug_mul), \n",
    "        'overwrite_output_dir': True, 'save_steps': -1, 'num_train_epochs': 50, 'train_batch_size': 100, 'eval_batch_size': 100, 'max_seq_length': 128, 'learning_rate': 0.002})\n",
    "        \n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.train_model(self.train_data)\n",
    "    \n",
    "    def eval_model(self):\n",
    "        self.result, self.model_outputs, self.wrong_predictions = self.model.eval_model(self.eval_data)\n",
    "\n",
    "    def test_model(self):\n",
    "        #self.to_predict = self.test_data.comment_text.apply(lambda x: x.replace('\\n', ' ')).tolist()\n",
    "        self.preds, outputs = self.model.predict(self.test_data)\n",
    "\n",
    "        sub_df = pd.DataFrame(outputs, columns = list(ml.data_ori.columns[4:-2]))\n",
    "\n",
    "        sub_df['id'] = test_df['id']\n",
    "        sub_df = sub_df[['id'].append(list(ml.data_ori.columns[4:-2]))]\n",
    "\n",
    "        sub_df.to_csv('outputs/submission.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                    text  \\\n0      tool to mount ndfs on linux tool to mount ndfs...   \n1      make configuration an interface the configurat...   \n2      df enhancement: performance and win xp support...   \n3      adding some uniformity/convenience to environm...   \n4      buffersize argument is ignored in filesystem.c...   \n...                                                  ...   \n43059  switch to v2 of the s3 li st objects api in s3...   \n43060  namenode connect t ime out in cluster with 65 ...   \n43061  eliminate needless uses of filesystem. exists,...   \n43062  dis pose of unnecess ary sasl s ervers the ipc...   \n43063  optimize and fix getfilestatus in s3a currentl...   \n\n                                                  labels  \n0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n1      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n3      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n...                                                  ...  \n43059  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n43060  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n43061  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n43062  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n43063  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n\n[43064 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "ml = ML_Classification(\"HADOOP\", \"Split\", 7, \"distilbert\")\n",
    "ml.refine_origin_data()\n",
    "ml.labels_to_int()\n",
    "print(ml.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  \\\n",
       "0      update j unit dependency simp le upd ate of th...   \n",
       "1      blockdecompressorstream get eof exception when...   \n",
       "2      native client: im plement hdfsmove and hdfscop...   \n",
       "3      w ritablecomparator ' s constructor should be ...   \n",
       "4      add build instruction for dock er toolbox inst...   \n",
       "...                                                  ...   \n",
       "25832  testviewfstrash assumes the user ' s home dire...   \n",
       "25833  reduce task gett ing map output ov er http sho...   \n",
       "25834  include librecordio as part of the rele ase no ne   \n",
       "25835  adding service - level authorization to hadoop...   \n",
       "25836  the real user name should be used by bin/hadoo...   \n",
       "\n",
       "                                                  labels  \n",
       "0      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "25832  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "25833  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25834  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25835  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25836  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "\n",
       "[25837 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>update j unit dependency simp le upd ate of th...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>blockdecompressorstream get eof exception when...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>native client: im plement hdfsmove and hdfscop...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>w ritablecomparator ' s constructor should be ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>add build instruction for dock er toolbox inst...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25832</th>\n      <td>testviewfstrash assumes the user ' s home dire...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>25833</th>\n      <td>reduce task gett ing map output ov er http sho...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>25834</th>\n      <td>include librecordio as part of the rele ase no ne</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>25835</th>\n      <td>adding service - level authorization to hadoop...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>25836</th>\n      <td>the real user name should be used by bin/hadoo...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>25837 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "ml.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          issuekey                                              title  \\\n",
       "1290   HADOOP-5353  add progress callback feature to the slow File...   \n",
       "464    HADOOP-2390  Document the user-controls for intermediate/ou...   \n",
       "4230  HADOOP-10399                          FileContext API for ACLs.   \n",
       "356    HADOOP-1772          Hadoop does not run in Cygwin in Windows    \n",
       "2547   HADOOP-7818  DiskChecker#checkDir should fail if the direct...   \n",
       "...            ...                                                ...   \n",
       "4979  HADOOP-11592  IPC error extraction fails \"getLength on unini...   \n",
       "4132  HADOOP-10226                      Help! My Hadoop doesn't work!   \n",
       "5386  HADOOP-12277  releasedocmaker index mode should create a rea...   \n",
       "4613  HADOOP-10979                       Auto-entries in hadoop_usage   \n",
       "1324   HADOOP-5520             Typo in diskQuota help  documentation    \n",
       "\n",
       "                                            description      component  \\\n",
       "1290  This is something only of relevance of people ...             fs   \n",
       "464   We should document the user-controls for compr...  documentation   \n",
       "4230  Add new methods to AbstractFileSystem and File...             fs   \n",
       "356   the hostname commands are slightly different i...        scripts   \n",
       "2547  DiskChecker#checkDir fails if a directory can'...           util   \n",
       "...                                                 ...            ...   \n",
       "4979  I'm  seeing {{java.lang.IllegalArgumentExcepti...            ipc   \n",
       "4132  I have installed hadoop but it it is failing  ...            bin   \n",
       "5386  The content should be the same, however, rathe...          yetus   \n",
       "4613  It would make adding common options to hadoop_...        scripts   \n",
       "1324  Minor typo in setSpaceQuota help documentation...  documentation   \n",
       "\n",
       "      auto-failover  azure  benchmarks  bin  build  conf  ...  test  tools  \\\n",
       "1290              0      0           0    0      0     0  ...     0      0   \n",
       "464               0      0           0    0      0     0  ...     0      0   \n",
       "4230              0      0           0    0      0     0  ...     0      0   \n",
       "356               0      0           0    0      0     0  ...     0      0   \n",
       "2547              0      0           0    0      0     0  ...     0      0   \n",
       "...             ...    ...         ...  ...    ...   ...  ...   ...    ...   \n",
       "4979              0      0           0    0      0     0  ...     0      0   \n",
       "4132              0      0           0    1      0     0  ...     0      0   \n",
       "5386              0      0           0    0      0     0  ...     0      0   \n",
       "4613              0      0           0    0      0     0  ...     0      0   \n",
       "1324              0      0           0    0      0     0  ...     0      0   \n",
       "\n",
       "      tools/distcp  tracing  trash  util  viewfs  yetus  \\\n",
       "1290             0        0      0     0       0      0   \n",
       "464              0        0      0     0       0      0   \n",
       "4230             0        0      0     0       0      0   \n",
       "356              0        0      0     0       0      0   \n",
       "2547             0        0      0     1       0      0   \n",
       "...            ...      ...    ...   ...     ...    ...   \n",
       "4979             0        0      0     0       0      0   \n",
       "4132             0        0      0     0       0      0   \n",
       "5386             0        0      0     0       0      1   \n",
       "4613             0        0      0     0       0      0   \n",
       "1324             0        0      0     0       0      0   \n",
       "\n",
       "                                                   text  \\\n",
       "1290  add progress callback feature to the slow file...   \n",
       "464   document the user-controls for intermediate/ou...   \n",
       "4230  filecontext api for acls. add new methods to a...   \n",
       "356   hadoop does not run in cygwin in windows  the ...   \n",
       "2547  diskchecker#checkdir should fail if the direct...   \n",
       "...                                                 ...   \n",
       "4979  ipc error extraction fails \"getlength on unini...   \n",
       "4132  help! my hadoop doesn't work! i have installed...   \n",
       "5386  releasedocmaker index mode should create a rea...   \n",
       "4613  auto-entries in hadoop_usage it would make add...   \n",
       "1324  typo in diskquota help  documentation  minor t...   \n",
       "\n",
       "                                                 labels  \n",
       "1290  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "464   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "4230  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "356   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2547  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "4979  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4132  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5386  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4613  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1324  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1230 rows x 43 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>issuekey</th>\n      <th>title</th>\n      <th>description</th>\n      <th>component</th>\n      <th>auto-failover</th>\n      <th>azure</th>\n      <th>benchmarks</th>\n      <th>bin</th>\n      <th>build</th>\n      <th>conf</th>\n      <th>...</th>\n      <th>test</th>\n      <th>tools</th>\n      <th>tools/distcp</th>\n      <th>tracing</th>\n      <th>trash</th>\n      <th>util</th>\n      <th>viewfs</th>\n      <th>yetus</th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1290</th>\n      <td>HADOOP-5353</td>\n      <td>add progress callback feature to the slow File...</td>\n      <td>This is something only of relevance of people ...</td>\n      <td>fs</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>add progress callback feature to the slow file...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>464</th>\n      <td>HADOOP-2390</td>\n      <td>Document the user-controls for intermediate/ou...</td>\n      <td>We should document the user-controls for compr...</td>\n      <td>documentation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>document the user-controls for intermediate/ou...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4230</th>\n      <td>HADOOP-10399</td>\n      <td>FileContext API for ACLs.</td>\n      <td>Add new methods to AbstractFileSystem and File...</td>\n      <td>fs</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>filecontext api for acls. add new methods to a...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>356</th>\n      <td>HADOOP-1772</td>\n      <td>Hadoop does not run in Cygwin in Windows</td>\n      <td>the hostname commands are slightly different i...</td>\n      <td>scripts</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hadoop does not run in cygwin in windows  the ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2547</th>\n      <td>HADOOP-7818</td>\n      <td>DiskChecker#checkDir should fail if the direct...</td>\n      <td>DiskChecker#checkDir fails if a directory can'...</td>\n      <td>util</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>diskchecker#checkdir should fail if the direct...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4979</th>\n      <td>HADOOP-11592</td>\n      <td>IPC error extraction fails \"getLength on unini...</td>\n      <td>I'm  seeing {{java.lang.IllegalArgumentExcepti...</td>\n      <td>ipc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ipc error extraction fails \"getlength on unini...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4132</th>\n      <td>HADOOP-10226</td>\n      <td>Help! My Hadoop doesn't work!</td>\n      <td>I have installed hadoop but it it is failing  ...</td>\n      <td>bin</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>help! my hadoop doesn't work! i have installed...</td>\n      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>5386</th>\n      <td>HADOOP-12277</td>\n      <td>releasedocmaker index mode should create a rea...</td>\n      <td>The content should be the same, however, rathe...</td>\n      <td>yetus</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>releasedocmaker index mode should create a rea...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4613</th>\n      <td>HADOOP-10979</td>\n      <td>Auto-entries in hadoop_usage</td>\n      <td>It would make adding common options to hadoop_...</td>\n      <td>scripts</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>auto-entries in hadoop_usage it would make add...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1324</th>\n      <td>HADOOP-5520</td>\n      <td>Typo in diskQuota help  documentation</td>\n      <td>Minor typo in setSpaceQuota help documentation...</td>\n      <td>documentation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>typo in diskquota help  documentation  minor t...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1230 rows × 43 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "ml.eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          issuekey                                              title  \\\n",
       "5553  HADOOP-12537  s3a: Add flag for session ID to allow Amazon S...   \n",
       "58      HADOOP-319  FileSystem \"close\" does not remove the closed ...   \n",
       "3833   HADOOP-9756                        Additional cleanup RPC code   \n",
       "4197  HADOOP-10349  TaskUmbilicalProtocol always uses TOKEN authen...   \n",
       "1213   HADOOP-5072  testSequenceFileGzipCodec won't pass without n...   \n",
       "...            ...                                                ...   \n",
       "111     HADOOP-683  bin/hadoop.sh doesn't work for /bin/dash (eg u...   \n",
       "3156   HADOOP-8695        TestPathData fails intermittently with JDK7   \n",
       "5912  HADOOP-13108  dynamic subcommands need a way to manipulate a...   \n",
       "1732   HADOOP-6562  FileContextSymlinkBaseTest should use FileCont...   \n",
       "2611   HADOOP-7901            Have Configuration use Read/Write Locks   \n",
       "\n",
       "                                            description component  \\\n",
       "5553  Amazon STS allows you to issue temporary acces...     fs/s3   \n",
       "58    The close methods of both DistributedFileSyste...        fs   \n",
       "3833  HADOOP-9754 already did good job to address mo...       ipc   \n",
       "4197  Since job tokens are always created. HADOOP-96...  security   \n",
       "1213  Somehow, SequenceFile requires native gzip cod...      test   \n",
       "...                                                 ...       ...   \n",
       "111   bin/hadoop.sh has a conditional which doesn't ...   scripts   \n",
       "3156  Failed tests:   testWithDirStringAndConf(org.a...      test   \n",
       "5912  It would be extremely useful to be able to man...   scripts   \n",
       "1732  FileContextSymlinkBaseTest should use FileCont...      test   \n",
       "2611  We can potentially improve performance by movi...      conf   \n",
       "\n",
       "      auto-failover  azure  benchmarks  bin  build  conf  ...  test  tools  \\\n",
       "5553              0      0           0    0      0     0  ...     0      0   \n",
       "58                0      0           0    0      0     0  ...     0      0   \n",
       "3833              0      0           0    0      0     0  ...     0      0   \n",
       "4197              0      0           0    0      0     0  ...     0      0   \n",
       "1213              0      0           0    0      0     0  ...     1      0   \n",
       "...             ...    ...         ...  ...    ...   ...  ...   ...    ...   \n",
       "111               0      0           0    0      0     0  ...     0      0   \n",
       "3156              0      0           0    0      0     0  ...     1      0   \n",
       "5912              0      0           0    0      0     0  ...     0      0   \n",
       "1732              0      0           0    0      0     0  ...     1      0   \n",
       "2611              0      0           0    0      0     1  ...     0      0   \n",
       "\n",
       "      tools/distcp  tracing  trash  util  viewfs  yetus  \\\n",
       "5553             0        0      0     0       0      0   \n",
       "58               0        0      0     0       0      0   \n",
       "3833             0        0      0     0       0      0   \n",
       "4197             0        0      0     0       0      0   \n",
       "1213             0        0      0     0       0      0   \n",
       "...            ...      ...    ...   ...     ...    ...   \n",
       "111              0        0      0     0       0      0   \n",
       "3156             0        0      0     0       0      0   \n",
       "5912             0        0      0     0       0      0   \n",
       "1732             0        0      0     0       0      0   \n",
       "2611             0        0      0     0       0      0   \n",
       "\n",
       "                                                   text  \\\n",
       "5553  s3a: add flag for session id to allow amazon s...   \n",
       "58    filesystem \"close\" does not remove the closed ...   \n",
       "3833  additional cleanup rpc code hadoop-9754 alread...   \n",
       "4197  taskumbilicalprotocol always uses token authen...   \n",
       "1213  testsequencefilegzipcodec won't pass without n...   \n",
       "...                                                 ...   \n",
       "111   bin/hadoop.sh doesn't work for /bin/dash (eg u...   \n",
       "3156  testpathdata fails intermittently with jdk7 fa...   \n",
       "5912  dynamic subcommands need a way to manipulate a...   \n",
       "1732  filecontextsymlinkbasetest should use filecont...   \n",
       "2611  have configuration use read/write locks we can...   \n",
       "\n",
       "                                                 labels  \n",
       "5553  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "58    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "3833  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4197  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1213  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "111   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3156  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5912  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1732  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2611  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1231 rows x 43 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>issuekey</th>\n      <th>title</th>\n      <th>description</th>\n      <th>component</th>\n      <th>auto-failover</th>\n      <th>azure</th>\n      <th>benchmarks</th>\n      <th>bin</th>\n      <th>build</th>\n      <th>conf</th>\n      <th>...</th>\n      <th>test</th>\n      <th>tools</th>\n      <th>tools/distcp</th>\n      <th>tracing</th>\n      <th>trash</th>\n      <th>util</th>\n      <th>viewfs</th>\n      <th>yetus</th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5553</th>\n      <td>HADOOP-12537</td>\n      <td>s3a: Add flag for session ID to allow Amazon S...</td>\n      <td>Amazon STS allows you to issue temporary acces...</td>\n      <td>fs/s3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>s3a: add flag for session id to allow amazon s...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>HADOOP-319</td>\n      <td>FileSystem \"close\" does not remove the closed ...</td>\n      <td>The close methods of both DistributedFileSyste...</td>\n      <td>fs</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>filesystem \"close\" does not remove the closed ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3833</th>\n      <td>HADOOP-9756</td>\n      <td>Additional cleanup RPC code</td>\n      <td>HADOOP-9754 already did good job to address mo...</td>\n      <td>ipc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>additional cleanup rpc code hadoop-9754 alread...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4197</th>\n      <td>HADOOP-10349</td>\n      <td>TaskUmbilicalProtocol always uses TOKEN authen...</td>\n      <td>Since job tokens are always created. HADOOP-96...</td>\n      <td>security</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>taskumbilicalprotocol always uses token authen...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1213</th>\n      <td>HADOOP-5072</td>\n      <td>testSequenceFileGzipCodec won't pass without n...</td>\n      <td>Somehow, SequenceFile requires native gzip cod...</td>\n      <td>test</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>testsequencefilegzipcodec won't pass without n...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>111</th>\n      <td>HADOOP-683</td>\n      <td>bin/hadoop.sh doesn't work for /bin/dash (eg u...</td>\n      <td>bin/hadoop.sh has a conditional which doesn't ...</td>\n      <td>scripts</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>bin/hadoop.sh doesn't work for /bin/dash (eg u...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3156</th>\n      <td>HADOOP-8695</td>\n      <td>TestPathData fails intermittently with JDK7</td>\n      <td>Failed tests:   testWithDirStringAndConf(org.a...</td>\n      <td>test</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>testpathdata fails intermittently with jdk7 fa...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>5912</th>\n      <td>HADOOP-13108</td>\n      <td>dynamic subcommands need a way to manipulate a...</td>\n      <td>It would be extremely useful to be able to man...</td>\n      <td>scripts</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>dynamic subcommands need a way to manipulate a...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1732</th>\n      <td>HADOOP-6562</td>\n      <td>FileContextSymlinkBaseTest should use FileCont...</td>\n      <td>FileContextSymlinkBaseTest should use FileCont...</td>\n      <td>test</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>filecontextsymlinkbasetest should use filecont...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2611</th>\n      <td>HADOOP-7901</td>\n      <td>Have Configuration use Read/Write Locks</td>\n      <td>We can potentially improve performance by movi...</td>\n      <td>conf</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>have configuration use read/write locks we can...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1231 rows × 43 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "ml.test_data"
   ]
  }
 ]
}