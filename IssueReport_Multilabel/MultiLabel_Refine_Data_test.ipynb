{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('tf-gpu-cuda8': conda)",
   "metadata": {
    "interpreter": {
     "hash": "592d2365cf8c69d9089d1741d1cf6e338245e4159eff9ececf626c2bb7405064"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "import os\n",
    "\n",
    "import inspect\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "import json, re, nltk, string\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDL_LABEL_NUM = 89\n",
    "JRA_LABEL_NUM = 142\n",
    "ISLANDORA_LABEL_NUM = 67\n",
    "INFRA_LABEL_NUM = 51\n",
    "HIVE_LABEL_NUM = 65\n",
    "HBASE_LABEL_NUM = 68\n",
    "HADOOP_LABEL_NUM = 37\n",
    "FCREPO_LABEL_NUM = 22\n",
    "CONF_LABEL_NUM = 128\n",
    "CB_LABEL_NUM = 64\n",
    "CASSANDRA_LABEL_NUM = 15\n",
    "BAM_LABEL_NUM = 96\n",
    "\n",
    "labels_Num = {'MDL': MDL_LABEL_NUM, \n",
    "    'JRA': JRA_LABEL_NUM, 'ISLANDORA': ISLANDORA_LABEL_NUM, \n",
    "    'INFRA': INFRA_LABEL_NUM, 'HIVE': HIVE_LABEL_NUM, 'HBASE': HBASE_LABEL_NUM, 'HADOOP': HADOOP_LABEL_NUM, 'FCREPO': FCREPO_LABEL_NUM, 'CONF': CONF_LABEL_NUM,\n",
    "    'CB': CB_LABEL_NUM, 'CASSANDRA': CASSANDRA_LABEL_NUM, 'BAM': BAM_LABEL_NUM\n",
    "    }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_Classification:\n",
    "    def __init__(self, dataset_name, augmenter_name, augment_size, nlp_model_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.labels_num = labels_Num[dataset_name]\n",
    "\n",
    "        if augmenter_name == 'OCR' or augmenter_name == 'Keyboard':\n",
    "            self.augmentation_type = 'char'\n",
    "        else:\n",
    "            self.augmentation_type = 'word'\n",
    "\n",
    "        self.augmenter_name = augmenter_name\n",
    "        self.aug_mul = augment_size\n",
    "        self.nlp_model_name = nlp_model_name\n",
    "        self.nlp_model = {'bert': 'bert-base-uncased', 'roberta': 'roberta-base', 'xlnet': 'xlnet-base-uncased', 'distilbert': 'distilbert-base-uncased', 'xlm': 'xlm-roberta-base', 'electra': 'google/electra-base-discriminator'}\n",
    "\n",
    "\n",
    "        # 데이터 위치 data location\n",
    "        self.data_location_ori = '../Dataset/Deepsoft_IssueData/{}.csv'.format(self.dataset_name)\n",
    "\n",
    "        self.data_location_aug = '../Dataset/Deepsoft_IssueData_Aug/{}_{}_{}.csv'.format(self.dataset_name, self.augmentation_type, self.augmenter_name)     \n",
    "        \n",
    "\n",
    "        # 데이터 변수 입력\n",
    "        self.data = pd.read_csv(self.data_location_aug) # 증강 데이터\n",
    "        self.data_ori = pd.read_csv(self.data_location_ori) # 원본 데이터\n",
    "        self.len_data = len(self.data_ori)\n",
    "        self.eval_index = []\n",
    "        self.test_index = []\n",
    "\n",
    "\n",
    "\n",
    "    def refine_origin_data(self):\n",
    "        data_ori = self.data_ori\n",
    "        \n",
    "        data_onehot = data_ori.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "        data_label = []\n",
    "        for i in range(len(data_onehot)):\n",
    "            data_label.append(list(data_onehot.iloc[i]))\n",
    "\n",
    "        # make 'data' value\n",
    "        data_text = pd.Series(list(data_ori[\"title\"] + ' ' + data_ori['description']), index = data_ori.index)\n",
    "        #data = data.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "        \n",
    "\n",
    "        refined_data = []\n",
    "        for item in data_text:\n",
    "            #1. Remove \\r \n",
    "            current_desc = item.replace('\\r', ' ')    \n",
    "            #2. Remove URLs\n",
    "            current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "            #4. Remove hex code\n",
    "            current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc) \n",
    "            #5. Change to lower case\n",
    "            current_desc = current_desc.lower()   \n",
    "            #6. Tokenize\n",
    "            #current_desc_tokens = tokenizer(current_desc, add_special_tokens= True)\n",
    "            #7. Strip trailing punctuation marks    \n",
    "            #current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]     \n",
    "            #8. Join the lists\n",
    "            #current_data = current_desc_filter\n",
    "            #current_data = list(filter(None, current_data))\n",
    "            refined_data.append(current_desc)\n",
    "\n",
    "        #data_ori = pd.DataFrame(data = {'text': refined_data, 'labels': data_label})\n",
    "        data_ori['text'] = refined_data\n",
    "        data_ori['labels'] = data_label\n",
    "        self.data_ori = data_ori\n",
    "        # 오리지날 데이터를 train, eval데이터로 분리\n",
    "\n",
    "        train_size = 0.6\n",
    "        test_size = 0.2\n",
    "        eval_size = 0.2\n",
    "        self.train_data_ori, self.eval_data_ori = train_test_split(data_ori, train_size = train_size)\n",
    "        self.eval_data_ori, self.test_data_ori = train_test_split(self.eval_data_ori, test_size = 0.5)\n",
    "        self.eval_data = self.eval_data_ori\n",
    "        self.train_data = self.train_data_ori\n",
    "        self.test_data = self.test_data_ori\n",
    "\n",
    "\n",
    "    \n",
    "    # 불러온 정제된 데이터 one hot을 str에서 list로 바꾸는 작업\n",
    "    def labels_to_int(self):\n",
    "        if self.aug_mul <= 1:\n",
    "            return        \n",
    "\n",
    "        data = self.data[: self.len_data * self.aug_mul] \n",
    "\n",
    "        refined_data = []\n",
    "        for item in data['text']:\n",
    "            #1. Remove \\r \n",
    "            current_desc = item.replace('\\r', ' ')    \n",
    "            #2. Remove URLs\n",
    "            current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "            #4. Remove hex code\n",
    "            current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc) \n",
    "            #5. Change to lower case\n",
    "            current_desc = current_desc.lower()   \n",
    "            #6. Tokenize\n",
    "            #current_desc_tokens = tokenizer(current_desc, add_special_tokens= True)\n",
    "            #7. Strip trailing punctuation marks    \n",
    "            #current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]     \n",
    "            #8. Join the lists\n",
    "            #current_data = current_desc_filter\n",
    "            #current_data = list(filter(None, current_data))\n",
    "            refined_data.append(current_desc)\n",
    "\n",
    "        data['text'] = refined_data\n",
    "\n",
    "        changeChar = ' [],'\n",
    "        for i in range(len(data)):\n",
    "            for chanChar in changeChar:\n",
    "                data['labels'][i] = data['labels'][i].replace(chanChar, '')\n",
    "            data['labels'][i] = list(data['labels'][i])\n",
    "            data['labels'][i] = list(map(int, data['labels'][i]))\n",
    "        \n",
    "        # 증강데이터의 train_data에서 evaluation부분 제거\n",
    "        eval_index_list = list(self.eval_data.index)\n",
    "        test_index_list = list(self.test_data.index)\n",
    "        \n",
    "        for aug_num in range(self.aug_mul):\n",
    "            iidf2 = [i + self.len_data* aug_num for i in eval_index_list]\n",
    "            self.eval_index = self.eval_index + iidf2\n",
    "            iidf3 = [x + self.len_data* aug_num for x in test_index_list]\n",
    "            self.test_index = self.test_index + iidf3\n",
    "\n",
    "\n",
    "        self.data = data\n",
    "        self.train_data = data.drop(self.eval_index)\n",
    "        self.train_data = self.train_data.drop(self.test_index)\n",
    "        self.train_data = self.train_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 모델 parameter 설정\n",
    "    def set_model(self): # epochs: 200, batch size: 100, learning rate 0.002\n",
    "        self.model = MultiLabelClassificationModel(self.nlp_model_name, self.nlp_model[self.nlp_model_name], num_labels = self.labels_num, \n",
    "        args = {'output_dir': '/data/a22106/Deepsoft_C_Multilabel/{}_{}_{}_{}/'.format(self.dataset_name, self.nlp_model_name, self.augmenter_name, self.aug_mul), \n",
    "        'overwrite_output_dir': True, 'save_steps': -1, 'num_train_epochs': 50, 'train_batch_size': 100, 'eval_batch_size': 100, 'max_seq_length': 128, 'learning_rate': 0.002})\n",
    "        \n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.train_model(self.train_data)\n",
    "    \n",
    "    def eval_model(self):\n",
    "        self.result, self.model_outputs, self.wrong_predictions = self.model.eval_model(self.eval_data)\n",
    "\n",
    "    def test_model(self):\n",
    "        self.to_predict = list(self.test_data['text'].apply(lambda x: x.replace('\\n', ' ')).tolist())\n",
    "        preds, outputs = self.model.predict(self.to_predict)\n",
    "\n",
    "        sub_df = pd.DataFrame(outputs, columns = list(ml.data_ori.columns[4:-2]))\n",
    "\n",
    "        sub_df.to_csv('outputs/submission.csv', index = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                    text  \\\n0      tool to mount ndfs on linux tool to mount ndfs...   \n1      make configuration an interface the configurat...   \n2      df enhancement: performance and win xp support...   \n3      adding some uniformity/convenience to environm...   \n4      buffersize argument is ignored in filesystem.c...   \n...                                                  ...   \n43059  switch to v2 of the s3 li st objects api in s3...   \n43060  namenode connect t ime out in cluster with 65 ...   \n43061  eliminate needless uses of filesystem. exists,...   \n43062  dis pose of unnecess ary sasl s ervers the ipc...   \n43063  optimize and fix getfilestatus in s3a currentl...   \n\n                                                  labels  \n0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n1      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n3      [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n...                                                  ...  \n43059  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n43060  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n43061  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n43062  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n43063  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n\n[43064 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "ml = ML_Classification(\"HADOOP\", \"Split\", 7, \"distilbert\")\n",
    "ml.refine_origin_data()\n",
    "ml.labels_to_int()\n",
    "print(ml.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                    text  \\\n",
       "0      te stfileappend2. testcomplexappe nd somet ime...   \n",
       "1      hadoop distribution tarball bundle some librar...   \n",
       "2      libhadoop. so: dlopen sho uld be better at loc...   \n",
       "3      namenode schema for httpfilesystem this issue ...   \n",
       "4      should run old version of unit tes ts to che c...   \n",
       "...                                                  ...   \n",
       "25832  inconsistent configuration values and incorrec...   \n",
       "25833  s3n files are not getting split by default  ru...   \n",
       "25834  p ort co nf servlet to dump running configurat...   \n",
       "25835  problem staging 0. 21. 0 artifacts to apache n...   \n",
       "25836  deprecate filesystem here's a jira to track de...   \n",
       "\n",
       "                                                  labels  \n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2      [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                  ...  \n",
       "25832  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...  \n",
       "25833  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "25834  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25835  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "25836  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "\n",
       "[25837 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>te stfileappend2. testcomplexappe nd somet ime...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>hadoop distribution tarball bundle some librar...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>libhadoop. so: dlopen sho uld be better at loc...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>namenode schema for httpfilesystem this issue ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>should run old version of unit tes ts to che c...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>25832</th>\n      <td>inconsistent configuration values and incorrec...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, ...</td>\n    </tr>\n    <tr>\n      <th>25833</th>\n      <td>s3n files are not getting split by default  ru...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n    </tr>\n    <tr>\n      <th>25834</th>\n      <td>p ort co nf servlet to dump running configurat...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>25835</th>\n      <td>problem staging 0. 21. 0 artifacts to apache n...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>25836</th>\n      <td>deprecate filesystem here's a jira to track de...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>25837 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "ml.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          issuekey                                              title  \\\n",
       "4752  HADOOP-11204  Fix incorrect property in hadoop-kms/src/main/...   \n",
       "4992  HADOOP-11609  Correct credential commands info in CommandsMa...   \n",
       "1770   HADOOP-6626                       NPE in TestIPC with kerberos   \n",
       "775    HADOOP-3406     Document controls for profiling maps & reduces   \n",
       "28      HADOOP-167  reducing the number of Configuration & JobConf...   \n",
       "...            ...                                                ...   \n",
       "2917   HADOOP-8308               Support cross-project Jenkins builds   \n",
       "1007   HADOOP-4217  TestLimitTasksPerJobTaskSchedule test is faili...   \n",
       "3594   HADOOP-9380                    Add totalLength to rpc response   \n",
       "3202   HADOOP-8756  Fix SEGV when libsnappy is in java.library.pat...   \n",
       "2750   HADOOP-8078  Add capability to turn on security in unit tests.   \n",
       "\n",
       "                                            description  \\\n",
       "4752  {{hadoop.security.keystore.JavaKeyStoreProvide...   \n",
       "4992     \"-i\" is not supported, so would you remove ...   \n",
       "1770  Running TestIPC with {{hadoop.security.authent...   \n",
       "775   HADOOP-2367 and further improvements added the...   \n",
       "28    Currently, Configuration and JobConf objects a...   \n",
       "...                                                 ...   \n",
       "2917  This issue is to change test-patch to run only...   \n",
       "1007  TestLimitTasksPerJobTaskSchedule test is faili...   \n",
       "3594                                               None   \n",
       "3202  We use {{System.loadLibrary(\"snappy\")}} from t...   \n",
       "2750  We should be able to start a kdc server for un...   \n",
       "\n",
       "                   component  auto-failover  azure  benchmarks  bin  build  \\\n",
       "4752                     kms              0      0           0    0      0   \n",
       "4992  documentation,security              0      0           0    0      0   \n",
       "1770                     ipc              0      0           0    0      0   \n",
       "775            documentation              0      0           0    0      0   \n",
       "28                      conf              0      0           0    0      0   \n",
       "...                      ...            ...    ...         ...  ...    ...   \n",
       "2917                   build              0      0           0    0      1   \n",
       "1007                    test              0      0           0    0      0   \n",
       "3594                     ipc              0      0           0    0      0   \n",
       "3202                  native              0      0           0    0      0   \n",
       "2750                    test              0      0           0    0      0   \n",
       "\n",
       "      conf  ...  test  tools  tools/distcp  tracing  trash  util  viewfs  \\\n",
       "4752     0  ...     0      0             0        0      0     0       0   \n",
       "4992     0  ...     0      0             0        0      0     0       0   \n",
       "1770     0  ...     0      0             0        0      0     0       0   \n",
       "775      0  ...     0      0             0        0      0     0       0   \n",
       "28       1  ...     0      0             0        0      0     0       0   \n",
       "...    ...  ...   ...    ...           ...      ...    ...   ...     ...   \n",
       "2917     0  ...     0      0             0        0      0     0       0   \n",
       "1007     0  ...     1      0             0        0      0     0       0   \n",
       "3594     0  ...     0      0             0        0      0     0       0   \n",
       "3202     0  ...     0      0             0        0      0     0       0   \n",
       "2750     0  ...     1      0             0        0      0     0       0   \n",
       "\n",
       "      yetus                                               text  \\\n",
       "4752      0  fix incorrect property in hadoop-kms/src/main/...   \n",
       "4992      0  correct credential commands info in commandsma...   \n",
       "1770      0  npe in testipc with kerberos running testipc w...   \n",
       "775       0  document controls for profiling maps & reduces...   \n",
       "28        0  reducing the number of configuration & jobconf...   \n",
       "...     ...                                                ...   \n",
       "2917      0  support cross-project jenkins builds this issu...   \n",
       "1007      0  testlimittasksperjobtaskschedule test is faili...   \n",
       "3594      0               add totallength to rpc response none   \n",
       "3202      0  fix segv when libsnappy is in java.library.pat...   \n",
       "2750      0  add capability to turn on security in unit tes...   \n",
       "\n",
       "                                                 labels  \n",
       "4752  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4992  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "1770  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "775   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "28    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "2917  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1007  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3594  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3202  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2750  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1230 rows x 43 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>issuekey</th>\n      <th>title</th>\n      <th>description</th>\n      <th>component</th>\n      <th>auto-failover</th>\n      <th>azure</th>\n      <th>benchmarks</th>\n      <th>bin</th>\n      <th>build</th>\n      <th>conf</th>\n      <th>...</th>\n      <th>test</th>\n      <th>tools</th>\n      <th>tools/distcp</th>\n      <th>tracing</th>\n      <th>trash</th>\n      <th>util</th>\n      <th>viewfs</th>\n      <th>yetus</th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4752</th>\n      <td>HADOOP-11204</td>\n      <td>Fix incorrect property in hadoop-kms/src/main/...</td>\n      <td>{{hadoop.security.keystore.JavaKeyStoreProvide...</td>\n      <td>kms</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>fix incorrect property in hadoop-kms/src/main/...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4992</th>\n      <td>HADOOP-11609</td>\n      <td>Correct credential commands info in CommandsMa...</td>\n      <td>\"-i\" is not supported, so would you remove ...</td>\n      <td>documentation,security</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>correct credential commands info in commandsma...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1770</th>\n      <td>HADOOP-6626</td>\n      <td>NPE in TestIPC with kerberos</td>\n      <td>Running TestIPC with {{hadoop.security.authent...</td>\n      <td>ipc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>npe in testipc with kerberos running testipc w...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>775</th>\n      <td>HADOOP-3406</td>\n      <td>Document controls for profiling maps &amp; reduces</td>\n      <td>HADOOP-2367 and further improvements added the...</td>\n      <td>documentation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>document controls for profiling maps &amp; reduces...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>HADOOP-167</td>\n      <td>reducing the number of Configuration &amp; JobConf...</td>\n      <td>Currently, Configuration and JobConf objects a...</td>\n      <td>conf</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>reducing the number of configuration &amp; jobconf...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2917</th>\n      <td>HADOOP-8308</td>\n      <td>Support cross-project Jenkins builds</td>\n      <td>This issue is to change test-patch to run only...</td>\n      <td>build</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>support cross-project jenkins builds this issu...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1007</th>\n      <td>HADOOP-4217</td>\n      <td>TestLimitTasksPerJobTaskSchedule test is faili...</td>\n      <td>TestLimitTasksPerJobTaskSchedule test is faili...</td>\n      <td>test</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>testlimittasksperjobtaskschedule test is faili...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3594</th>\n      <td>HADOOP-9380</td>\n      <td>Add totalLength to rpc response</td>\n      <td>None</td>\n      <td>ipc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>add totallength to rpc response none</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3202</th>\n      <td>HADOOP-8756</td>\n      <td>Fix SEGV when libsnappy is in java.library.pat...</td>\n      <td>We use {{System.loadLibrary(\"snappy\")}} from t...</td>\n      <td>native</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>fix segv when libsnappy is in java.library.pat...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2750</th>\n      <td>HADOOP-8078</td>\n      <td>Add capability to turn on security in unit tests.</td>\n      <td>We should be able to start a kdc server for un...</td>\n      <td>test</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>add capability to turn on security in unit tes...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1230 rows × 43 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "ml.eval_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          issuekey                                              title  \\\n",
       "3503   HADOOP-9242  Duplicate surefire plugin config in hadoop-common   \n",
       "5665  HADOOP-12697  IPC retry policies should recognise that SASL ...   \n",
       "4512  HADOOP-10825  Refactor class creation logic in Configuration...   \n",
       "1479   HADOOP-6140  DistributedCache.addArchiveToClassPath doesn't...   \n",
       "5684  HADOOP-12745           stop shelltest profile active by default   \n",
       "...            ...                                                ...   \n",
       "3821   HADOOP-9744                            TestNetUtils test fails   \n",
       "1527   HADOOP-6217                      Hadoop Doc Split: Common Docs   \n",
       "4520  HADOOP-10834                      Typo in CredentialShell usage   \n",
       "3239   HADOOP-8809  RPMs should skip useradds if the users already...   \n",
       "5679  HADOOP-12731                Remove useless boxing/unboxing code   \n",
       "\n",
       "                                            description      component  \\\n",
       "3503  Unfortunately in HADOOP-9217 a duplicated conf...           test   \n",
       "5665  SLIDER-1050 shows that if you don't have the r...            ipc   \n",
       "4512  This first patch refactors class creation insi...           conf   \n",
       "1479  addArchiveToClassPath is a method of Distribut...             fs   \n",
       "5684  the shelltest profile is enabled by default.  ...          build   \n",
       "...                                                 ...            ...   \n",
       "3821  - testNormalizeHostName(org.apache.hadoop.net....           test   \n",
       "1527  Hadoop Doc Split: Common Docs    Please note t...  documentation   \n",
       "4520  There is a typo in one of the informational me...       security   \n",
       "3239  The hadoop.spec preinstall script creates user...        scripts   \n",
       "5679  There are lots of places where useless boxing/...    performance   \n",
       "\n",
       "      auto-failover  azure  benchmarks  bin  build  conf  ...  test  tools  \\\n",
       "3503              0      0           0    0      0     0  ...     1      0   \n",
       "5665              0      0           0    0      0     0  ...     0      0   \n",
       "4512              0      0           0    0      0     1  ...     0      0   \n",
       "1479              0      0           0    0      0     0  ...     0      0   \n",
       "5684              0      0           0    0      1     0  ...     0      0   \n",
       "...             ...    ...         ...  ...    ...   ...  ...   ...    ...   \n",
       "3821              0      0           0    0      0     0  ...     1      0   \n",
       "1527              0      0           0    0      0     0  ...     0      0   \n",
       "4520              0      0           0    0      0     0  ...     0      0   \n",
       "3239              0      0           0    0      0     0  ...     0      0   \n",
       "5679              0      0           0    0      0     0  ...     0      0   \n",
       "\n",
       "      tools/distcp  tracing  trash  util  viewfs  yetus  \\\n",
       "3503             0        0      0     0       0      0   \n",
       "5665             0        0      0     0       0      0   \n",
       "4512             0        0      0     0       0      0   \n",
       "1479             0        0      0     0       0      0   \n",
       "5684             0        0      0     0       0      0   \n",
       "...            ...      ...    ...   ...     ...    ...   \n",
       "3821             0        0      0     0       0      0   \n",
       "1527             0        0      0     0       0      0   \n",
       "4520             0        0      0     0       0      0   \n",
       "3239             0        0      0     0       0      0   \n",
       "5679             0        0      0     0       0      0   \n",
       "\n",
       "                                                   text  \\\n",
       "3503  duplicate surefire plugin config in hadoop-com...   \n",
       "5665  ipc retry policies should recognise that sasl ...   \n",
       "4512  refactor class creation logic in configuration...   \n",
       "1479  distributedcache.addarchivetoclasspath doesn't...   \n",
       "5684  stop shelltest profile active by default the s...   \n",
       "...                                                 ...   \n",
       "3821  testnetutils test fails - testnormalizehostnam...   \n",
       "1527  hadoop doc split: common docs hadoop doc split...   \n",
       "4520  typo in credentialshell usage there is a typo ...   \n",
       "3239  rpms should skip useradds if the users already...   \n",
       "5679  remove useless boxing/unboxing code there are ...   \n",
       "\n",
       "                                                 labels  \n",
       "3503  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5665  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4512  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1479  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "5684  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "3821  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1527  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "4520  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3239  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5679  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[1231 rows x 43 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>issuekey</th>\n      <th>title</th>\n      <th>description</th>\n      <th>component</th>\n      <th>auto-failover</th>\n      <th>azure</th>\n      <th>benchmarks</th>\n      <th>bin</th>\n      <th>build</th>\n      <th>conf</th>\n      <th>...</th>\n      <th>test</th>\n      <th>tools</th>\n      <th>tools/distcp</th>\n      <th>tracing</th>\n      <th>trash</th>\n      <th>util</th>\n      <th>viewfs</th>\n      <th>yetus</th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3503</th>\n      <td>HADOOP-9242</td>\n      <td>Duplicate surefire plugin config in hadoop-common</td>\n      <td>Unfortunately in HADOOP-9217 a duplicated conf...</td>\n      <td>test</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>duplicate surefire plugin config in hadoop-com...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>5665</th>\n      <td>HADOOP-12697</td>\n      <td>IPC retry policies should recognise that SASL ...</td>\n      <td>SLIDER-1050 shows that if you don't have the r...</td>\n      <td>ipc</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>ipc retry policies should recognise that sasl ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4512</th>\n      <td>HADOOP-10825</td>\n      <td>Refactor class creation logic in Configuration...</td>\n      <td>This first patch refactors class creation insi...</td>\n      <td>conf</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>refactor class creation logic in configuration...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1479</th>\n      <td>HADOOP-6140</td>\n      <td>DistributedCache.addArchiveToClassPath doesn't...</td>\n      <td>addArchiveToClassPath is a method of Distribut...</td>\n      <td>fs</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>distributedcache.addarchivetoclasspath doesn't...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>5684</th>\n      <td>HADOOP-12745</td>\n      <td>stop shelltest profile active by default</td>\n      <td>the shelltest profile is enabled by default.  ...</td>\n      <td>build</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>stop shelltest profile active by default the s...</td>\n      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3821</th>\n      <td>HADOOP-9744</td>\n      <td>TestNetUtils test fails</td>\n      <td>- testNormalizeHostName(org.apache.hadoop.net....</td>\n      <td>test</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>testnetutils test fails - testnormalizehostnam...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1527</th>\n      <td>HADOOP-6217</td>\n      <td>Hadoop Doc Split: Common Docs</td>\n      <td>Hadoop Doc Split: Common Docs    Please note t...</td>\n      <td>documentation</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>hadoop doc split: common docs hadoop doc split...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4520</th>\n      <td>HADOOP-10834</td>\n      <td>Typo in CredentialShell usage</td>\n      <td>There is a typo in one of the informational me...</td>\n      <td>security</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>typo in credentialshell usage there is a typo ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3239</th>\n      <td>HADOOP-8809</td>\n      <td>RPMs should skip useradds if the users already...</td>\n      <td>The hadoop.spec preinstall script creates user...</td>\n      <td>scripts</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>rpms should skip useradds if the users already...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>5679</th>\n      <td>HADOOP-12731</td>\n      <td>Remove useless boxing/unboxing code</td>\n      <td>There are lots of places where useless boxing/...</td>\n      <td>performance</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>remove useless boxing/unboxing code there are ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1231 rows × 43 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "ml.test_data"
   ]
  }
 ]
}