{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd02cafbbdf5f0c485b1a1935b2358d1e2de8ca6414272176d54707d0003e55811a",
   "display_name": "Python 3.7.9 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random \n",
    "import transformers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from simpletransformers.classification import MultiLabelClassificationModel\n",
    "import os\n",
    "import torch\n",
    "torch.cuda.device(1)\n",
    "\n",
    "import inspect\n",
    "import re\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'HADOOP'\n",
    "\n",
    "MDL_LABEL_NUM = 89\n",
    "JRA_LABEL_NUM = 142\n",
    "ISLANDORA_LABEL_NUM = 67\n",
    "INFRA_LABEL_NUM = 51\n",
    "HIVE_LABEL_NUM = 65\n",
    "HBASE_LABEL_NUM = 68\n",
    "HADOOP_LABEL_NUM = 37\n",
    "FCREPO_LABEL_NUM = 22\n",
    "CONF_LABEL_NUM = 128\n",
    "CB_LABEL_NUM = 64\n",
    "CASSANDRA_LABEL_NUM = 15\n",
    "BAM_LABEL_NUM = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ML_Classification:\n",
    "\n",
    "    labels_Num = {'MDL': MDL_LABEL_NUM, \n",
    "    'JRA': JRA_LABEL_NUM, 'ISLANDORA': ISLANDORA_LABEL_NUM, \n",
    "    'INFRA': INFRA_LABEL_NUM, 'HIVE': HIVE_LABEL_NUM, 'HBASE': HBASE_LABEL_NUM, 'HADOOP': HADOOP_LABEL_NUM, 'FCREPO': FCREPO_LABEL_NUM, 'CONF': CONF_LABEL_NUM,\n",
    "     'CB': CB_LABEL_NUM, 'CASSANDRA': CASSANDRA_LABEL_NUM, 'BAM': BAM_LABEL_NUM\n",
    "     }\n",
    "    \n",
    "    def __init__(self, dataset_name = dataset_name, labels_num = labels_Num[dataset_name]):\n",
    "        self.nlp_model = {'bert': 'bert-base-cased', 'roberta': 'roberta-base', 'xlnet': 'xlnet-base-cased', 'distilbert': 'distilbert-base-cased', 'xlm': 'xlm-roberta-base', 'electra': 'google/electra-base-discriminator'}\n",
    "        self.nlp_model_name = 'distilbert'\n",
    "        self.dataset_name = dataset_name\n",
    "        self.labels_num = labels_num\n",
    "\n",
    "#       self.data_location = '/home/a22106/python_practice/component_classification/DeepSoft-C/Dataset/Data/{}.csv'.format(self.dataset_name)\n",
    "        \n",
    "        # 데이터 위치\n",
    "        self.data_location_aug = 'D:\\\\OneDrive\\\\Deepsoft_C_classification\\\\HADOOP_aug_mul3.csv'\n",
    "        self.data_location_ori = 'D:\\\\OneDrive\\\\Deepsoft_C_classification\\\\HADOOP.csv'\n",
    "\n",
    "        # 데이터 변수 입력\n",
    "        self.data = pd.read_csv(self.data_location_aug) # 증강 데이터\n",
    "        self.data_ori = pd.read_csv(self.data_location_ori) # 원본 데이터\n",
    "        self.eval_index = []\n",
    "\n",
    "\n",
    "    def refine_origin_data(self):\n",
    "        data = self.data_ori\n",
    "        \n",
    "        data_onehot = data.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "        data_label = []\n",
    "        for i in range(len(data_onehot)):\n",
    "            data_label.append(list(data_onehot.iloc[i]))\n",
    "\n",
    "        # make 'data' value\n",
    "        data_text = pd.Series(list(data[\"title\"] + ' ' + data['description']), index = data.index)\n",
    "\n",
    "        #data = data.drop(columns = ['issuekey', 'title', 'description', 'component'])\n",
    "        self.data_ori = pd.DataFrame(data = {'text': data_text, 'labels': data_label})\n",
    "\n",
    "        # 오리지날 데이터를 train, eval데이터로 분리\n",
    "        self.train_data_ori, self.eval_data_ori = train_test_split(self.data_ori, test_size = 0.3)\n",
    "        self.eval_data = self.eval_data_ori\n",
    "    \n",
    "\n",
    "    # 불러온 정제된 데이터 one hot을 str에서 list로 바꾸는 작업\n",
    "    def labels_to_int(self):\n",
    "        data = self.data\n",
    "        changeChar = ' [],'\n",
    "        for i in range(len(data)):\n",
    "            for chanChar in changeChar:\n",
    "                data['labels'][i] = data['labels'][i].replace(chanChar, '')\n",
    "            data['labels'][i] = list(data['labels'][i])\n",
    "            data['labels'][i] = list(map(int, data['labels'][i]))\n",
    "        \n",
    "        # 증강데이터의 train_data에서 evaluation부분 제거\n",
    "        \n",
    "        eval_index_list = list(self.eval_data.index)\n",
    "        \n",
    "        for aug_num in range(4):\n",
    "            iidf2 = [i + 6152* aug_num for i in eval_index_list]\n",
    "            self.eval_index = self.eval_index + iidf2\n",
    "\n",
    "            #if aug_num == 0:\n",
    "            #    eval_index = self.eval_data.index.append(self.eval_data_ori.index +(6152*aug_num))\n",
    "            #else:\n",
    "            #    for i in self.eval_data.index:\n",
    "            #        self.eval_data.index.append(self.eval_data_ori.index +(6152*aug_num)\n",
    "#                eval_index = eval_index.append(self.eval_data_ori.index +(6152*aug_num))\n",
    "\n",
    "        self.train_data = data.drop(self.eval_index)\n",
    "\n",
    "        #print(self.train_data, self.eval_index.sort())\n",
    "\n",
    "# train, evaluation 데이터로 나누기 // 사용 안함\n",
    "    def split_data_eval(self):\n",
    "        data = self.data\n",
    "        train_data_ori, eval_data_ori = train_test_split(self.data_ori, test_size = 0.3, random_state = 32)\n",
    "\n",
    "        eval_data_indexList = eval_data_ori.index.append(eval_data_ori.index * 2)\n",
    "        data = data.drop(eval_data_indexList)\n",
    "        \n",
    "        self.train_data, self.eval_data = train_test_split(data, test_size = 0.3, random_state = 32)\n",
    "\n",
    "\n",
    "# 모델 parameter 설정\n",
    "    def set_model(self):\n",
    "        self.model = MultiLabelClassificationModel(self.nlp_model_name, self.nlp_model[self.nlp_model_name], num_labels = self.labels_num, args = {'output_dir': 'D:/OneDrive/Deepsoft_C_classification/Outputs/HADOOP/{}/outputs20/'.format(self.nlp_model_name), 'overwrite_output_dir': False, 'num_train_epochs':40, 'gradient_accumulation_steps': 2, 'max_seq_length': 128, 'learning_rate': 5e-5})\n",
    "\n",
    "    def train_model(self):\n",
    "        self.model.train_model(self.train_data)\n",
    "    \n",
    "    def eval_model(self):\n",
    "        self.result, self.model_outputs, self.wrong_predictions = self.model.eval_model(self.eval_data)\n",
    "    \n",
    "    def predict(self):\n",
    "        self.preds, self.outputs = self.model.predict(self.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = ML_Classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ml.refine_origin_data()\n",
    "ml.labels_to_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                   text  \\\n",
       "0     tool to mount ndfs on linux tool to mount ndfs...   \n",
       "1     make Configuration an interface The Configurat...   \n",
       "2     DF enhancement: performance and win XP support...   \n",
       "3     Adding some uniformity/convenience to environm...   \n",
       "4     bufferSize argument is ignored in FileSystem.c...   \n",
       "...                                                 ...   \n",
       "6147  Switch to v2 of the S3 List Objects API in S3A...   \n",
       "6148  namenode connect time out in cluster with 65 m...   \n",
       "6149  Eliminate needless uses of FileSystem.exists, ...   \n",
       "6150  Dispose of unnecessary SASL servers The IPC se...   \n",
       "6151  Optimize and fix getFileStatus in S3A Currentl...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "1     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "3     [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "6147  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "6148  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6149  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n",
       "6150  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "6151  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...  \n",
       "\n",
       "[6152 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>tool to mount ndfs on linux tool to mount ndfs...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>make Configuration an interface The Configurat...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DF enhancement: performance and win XP support...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Adding some uniformity/convenience to environm...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>bufferSize argument is ignored in FileSystem.c...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6147</th>\n      <td>Switch to v2 of the S3 List Objects API in S3A...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n    </tr>\n    <tr>\n      <th>6148</th>\n      <td>namenode connect time out in cluster with 65 m...</td>\n      <td>[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>6149</th>\n      <td>Eliminate needless uses of FileSystem.exists, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>6150</th>\n      <td>Dispose of unnecessary SASL servers The IPC se...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>6151</th>\n      <td>Optimize and fix getFileStatus in S3A Currentl...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>6152 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "ml.data[:6152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                    text  \\\n10735  Typo in com.getPasswordFromCredentialProviders...   \n18962  add lib#getHomeDirectory() method Their standa...   \n18325  Authenticate with Kerberos credentials when re...   \n6204   nicer lot of errors for distcp The unformatted...   \n7979   IPC doesn't want handle IOEs thrown behind soc...   \n...                                                  ...   \n3059   Potential deadlock in IPC This cycle (see atta...   \n2945   Changes to support Kerberos with non Sun JVM (...   \n21555  Fix issues faced by developers Harsh recently ...   \n23004  incorrect lookup in remote.c There is some inc...   \n3504   Some improvements to the mailing lists webpage...   \n\n                                                  labels  \n10735  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n18962  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n18325  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n6204   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n7979   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n...                                                  ...  \n3059   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2945   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n21555  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n23004  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3504   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n\n[17224 rows x 2 columns]\n                                                   text  \\\n2290  Add another IOUtils#copyBytes method Common si...   \n32    comparators of integral writable types are not...   \n894   [HOD] Remove dfs.client.buffer.dir generation,...   \n1563  Fix FileContext to allow both recursive and no...   \n611   [HOD] Hod should not check for the pkgs direct...   \n...                                                 ...   \n5051  Add Redirecting WebSSO behavior with JWT Token...   \n1576  FsShell -text should work on filesystems other...   \n5135  Incorrect arguments to sizeof in DomainSocket....   \n2581  Move the support for multiple protocols to low...   \n493   sequence file does not detect corruption in ke...   \n\n                                                 labels  \n2290  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n32    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n894   [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n1563  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n611   [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n...                                                 ...  \n5051  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n1576  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...  \n5135  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2581  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n493   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n\n[1846 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(shuffle(ml.train_data))\n",
    "print(ml.eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bufferSize This is returned in FileSystem.create(File, boolean, overwrite) org.apache.hadoop.fs.FileSystem.create(File File, boolean overwrite, int bufferSize)  ignores the input parameter bufferSize. It passes further down the internal pipeline, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config.  the although is probably affected by changes, see   org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, int done)  The attached patch would fix some.\nbufferSize argument is ignored in FileSystem.create(File, boolean, int) org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize)  ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config.  MapReduce although is probably affected by that, see   org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done)  The attached patch would fix it.\n"
     ]
    }
   ],
   "source": [
    "i = 4\n",
    "\n",
    "print(ml.train_data['text'][i + 6152])\n",
    "print(ml.train_data['text'][i])\n",
    "#print(ml.train_data.index.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}